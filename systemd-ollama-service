[Unit]
Description=Ollama Server
After=network.target

[Service]
ExecStart=/usr/local/bin/ollama serve
Environment="OLLAMA_HOST=0.0.0.0:11435"            # Listen on all interfaces
Environment="CUDA_VISIBLE_DEVICES=0"                # Use all available GPUs (can be adjusted if multiple GPUs)
Environment="OLLAMA_ORIGINS=*"                     # Set allowed origins
Environment="OLLAMA_GPU_OVERHEAD=0"                # Use full GPU power
Environment="OLLAMA_CONTEXT_LENGTH=8192"           # Larger context length (if memory allows)
Environment="OLLAMA_MAX_QUEUE=1000"                # Increase queue capacity for faster processing
Environment="OLLAMA_MAX_LOADED_MODELS=1"           # Load more models simultaneously
Environment="OLLAMA_NUM_PARALLEL=0"                # Increase the number of parallel processes
User=root
Restart=always

# Maximum resource usage
CPUQuota=100%                 # Use 100% of CPU
MemoryLimit=infinity          # Use all available memory
TasksMax=infinity             # Use as many tasks as needed
LimitNOFILE=100000            # Increase the number of open files
LimitNPROC=infinity           # No limit on the number of processes
LimitFSIZE=infinity           # No limit on file size
LimitCORE=infinity            # No limit on core dump files
RestartSec=0                  # No delay between restarts (if applicable)

[Install]
WantedBy=multi-user.target
